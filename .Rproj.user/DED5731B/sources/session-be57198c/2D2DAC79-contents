---
title: "Arctic Fox Seasonal Molting"
author: "Alisse Metge"
date: "2024-11-18"
output: html_document
---

# Introduction

The Arctic fox (*Vulpes lagopus*), like many species who live with seasonal snow, undergo a molt from denser fur in the winter to a shorter coat in the summer. Additionally, Arctic foxes occur in two color morphs, white and blue. During the molt, the white color morph changes from fully white in the winter to dorsally brownish in the summer, while the blue color morph varies from slightly lighter in winter to darker brown-gray in summer. Examples of these color morphs and seasonal pelage are shown in Fig. 1, taken from Laporte-Devylder et al, 2022.



```{r molt_image, echo = FALSE, eval = TRUE, fig.cap="Fig. 1 Arctic foxes of the white (top) and blue (bottom) color morphs, from winter (100%) to summer (0%) coats.", fig.align='center', out.width='100%'}
knitr::include_graphics("Documents/ArcticFoxMolt.jpg")
```



## Overview of project

I based my project on a study that looked at the timing and rate of seasonal molting of Arctic foxes (Laporte-Devylder et al, 2022). The dataset for this study consists of camera trap observations of Arctic foxes across 22 sites in Norway during their molting season, from 2011-2018. The dataset also includes average seasonal temperature each year (averaged across all sites) and site-specific measures of snow depth, snow continuity (number of days with snow on the ground) and a rodent density index for each site for each year sampled. (Not all sites were sampled each year.) The authors were interested in examining the timing of the molt in relation to environmental condtions.

I downloaded two csv files from this Arctic fox dataset. Both tables contain too many categories of information and a lot of duplicated information, so my project was to clean up the data and reorganize them into a relational database. I also created a few visualizations of some of the data to begin to explore general trends in the data.

## Format of original data

The first original table (morph_phenology.csv) lists individual foxes and their color morph with the site, year, several measurements of environmental conditions at that site and year, and columns summarizing the start, median, and ending dates of the molt for that individual that year. This means that every individual fox observation also lists again all the measured environmental conditions for the corresponding site and year – that’s a lot of duplication! Similarly, the second original table (seasonal_moulting_phenology.csv) contains all those same data again, but with additional rows for each individual, listing all molt observations and dates for that individual, rather than a summary of the start-median-end of the molt. And again, all the site-year conditions are relisted with every entry (in addition to the molt data being duplicated). These highly duplicated data are just screaming to be reorganized into a relational database.

For reference, here is a list of the columns contained in the two original csv files:

```{r csv_chart, echo = FALSE, eval = TRUE, fig.cap="", fig.align='center', out.width='60%'}
knitr::include_graphics("Documents/OriginalCSVs.jpg")
```

# Creating a Relational Database

## Structure of the relational database

I created a relational database, as outlined in the following ERD:

```{r ERD, echo = FALSE, eval = TRUE, fig.cap="", fig.align='center', out.width='150%'}
knitr::include_graphics("Documents/ArcticFoxERD.jpg")
```

A parent table of sites assigns a short (3 characters) site ID to each site name (which are long Norwegian names with special characters). Another parent table lists each individual fox and its color morph (white or blue), which does not change. An intermediate table contains the environmental conditions at each site each year, and the final table contains all the molt observations of individual foxes. For these last two tables, the site_year_id and molt_id primary keys are autogenerated integers.

## Creating the empty database

The relational database was constructed as follows. First, load the necessary R packages:

```{r load_packages, echo = TRUE, eval = TRUE}
# Load packages

library(DBI)
library(RSQLite)
```

Then, establish a database connection to create the database (note that this will be stored in whatever directory you are working in):

```{r db_setup, echo = TRUE, eval = FALSE}
# Create database

ArcticFox_db <- dbConnect(RSQLite::SQLite(), "ArcticFox_db.db")
```

Next, we can construct each table in the database using the RSQLite package. Here is the code to create the sites table, with columns for site id and the original site name:

```{r sites_structure, echo = TRUE, eval = FALSE}
dbExecute(ArcticFox_db,
         "CREATE TABLE sites (
            site_id char(3) NOT NULL PRIMARY KEY,
            site varchar(40)
          );")
```

Code for the individuals table, which contains columns for individual ID and the morph (which does not change for each individual). Because there are only two possible entries for color morph, I constrained the characters using `CHECK` in the table.:

```{r individuals_structure, echo = TRUE, eval = FALSE}
dbExecute(ArcticFox_db, 
          "CREATE TABLE individuals (
indiv_ID varchar(12) NOT NULL PRIMARY KEY,
morph char(1) CHECK (morph IN ('W', 'B'))
);")
```

The site_year_conditions table will contain columns for the site id, year, and the environmental conditions (rodent index, temperature, snow depth, snow continuity). Note that because there are only four possible values for the rodent index, I constrained the values using `CHECK` in the table. It will also auto generate an integer for the site-year ID, which is the primary key:

```{r siteyear_structure, echo = TRUE, eval = FALSE}
dbExecute(ArcticFox_db,
          "CREATE TABLE site_year_conditions (
site_year_id integer PRIMARY KEY AUTOINCREMENT,
site_id char(3),
year integer,
rodent integer CHECK (rodent IN ('1', '2', '3', '4')),
temperature real,
snow_depth real,
snow_continuous integer,
FOREIGN KEY (site_id) REFERENCES sites(site_id)
);")
```

And finally, the molt observation table, which contains columns for site-year ID, individual ID, the date, and the molt score. It will also generate an integer for the molt ID. Note that in the original data, date is recorded as the number of days since January 1, so I set it here to an integer. The molt score is the percentage of winter coat remaining (0, 5, 25, 50, 75, 95), so it will also be set as an integer. Here is the code:

```{r motl_structure, echo = TRUE, eval = FALSE}
dbExecute(ArcticFox_db,
          "CREATE TABLE molt_observations (
molt_id integer PRIMARY KEY AUTOINCREMENT,
site_year_id integer,
indiv_ID varchar(12),
date integer,
moult_score integer,
FOREIGN KEY (site_year_id) REFERENCES site_year_conditions(site_year_id),
FOREIGN KEY (indiv_ID) REFERENCES individuals(indiv_ID)
);")
```

# Populating the database

Because data were originally in two large, unruly tables with much duplication, I needed to clean these up and select the right columns to go in each table. Fist, I needed to load the data (note that I had already created the csv for sites, as described in the next section):

```{r load_data, echo = TRUE, eval = FALSE}
# Load the data

phenology <- read.csv("raw_data/morph_phenology.csv")
moult <- read.csv("raw_data/seasonal_moulting_phenology.csv")
sites <- read.csv("raw_data/sites.csv")
```

## Sites table

I created this table by hand in Excel, using the first three characters of each site name as its site ID. In the few cases where site names were partially repeated (such as north and south locations for Kjelsungbandet), I used the first two letters and the appropriate designation (e.g. KjS and KjN). (If I had not wanted these IDs to consist of part of the original names, I could have selected the sites column from one of the other tables, grouped by unique site names and then autogenerated an integer ID as the primary key).

The code for populating the sites table in the database was:

```{r sites, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "sites", sites, append = TRUE)
```

## Individuals table

This can be generated by selecting only the individual ID and morph columns from the seasonal_moulting_phenology.csv, and grouping by unique IDs. The code for this was as follows:

```{r indiv, echo = TRUE, eval = FALSE}
individuals <- phenology %>% 
  select(indiv_ID, morph)%>%
  distinct()
```

To populate the table in the database, I used this code:

```{r individuals, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "individuals", individuals, append = TRUE)
```

## Site-year conditions table

The phenology table was combined with the sites table using `left_join`, because I needed the site id. Then a new temporary column (site_id_year) was created with the `mutate` function to combine the site_id and year together. Next, I eliminated unwanted columns (morph, indiv_ID, start_95, median_50, end_0) using `select` and asked for `distinct` combinations of the remaining columns. Then, because I now had all the distinct site and year combinations, I could eliminate the site and temporary site_id_year columns with `select`and then `relocate` the columns in the preferred order. This leaves us with a table that contains the site_id, year, rodent, temperature, snow-depth, and snow_continuous. Here is the code:

```{r site_year, echo = TRUE, eval = FALSE}
site_year_conditions <- phenology %>%
  left_join(sites, by = "site") %>%
  mutate(site_id_year = paste(site_id, year, sep = "_")) %>%
  select(-c(morph, indiv_ID, start_95, median_50, end_0)) %>%
  distinct() %>%
  select(-site, -site_id_year) %>%
  relocate(site_id, .before = year)
```

I then populated the table in the database:

```{r site_year_conditions, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "site_year_conditions", 
             site_year_conditions, append = TRUE)
```

## Molt obeservations table

At this stage, I ran into the issue of having two different entities called "site_year_conditions," one of which was the temporary table I created, without a primary key, while the desired one was the table I populated in the database, including the primary key. In order to ensure that the latter would be what was referenced in my subsequent script, I ran the following code to overwrite the original object:

```{r overwrite, echo = TRUE, eval = FALSE}
site_year_conditions <- dbGetQuery(ArcticFox_db, 
                                   "SELECT * FROM site_year_conditions;")
```

I was then ready to create the molt observations table. This required me to `left_join` to both the sites table and the site_year_conditions table to pick up the necessary columns. I then used `select` to choose the desired columns, as follows:

```{r molt, echo = TRUE, eval = FALSE}
molt_observations <- moult %>%
  left_join(sites, by = "site") %>%
  left_join(site_year_conditions) %>%
  select(site_year_id, indiv_ID, date, moult_score)
```

To populate the table in the database, we can use:

```{r molt_observations, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "molt_observations", 
             molt_observations, append = TRUE)
```

## Check the data

Now all four tables of the database are populated and ready to be used for data analysis. We can check to ensure that each of the tables was populated correctly with the following code:

```{r check, echo = TRUE, eval = TRUE}
# Confirm that package is loaded and connect to the database

library(DBI)

ArcticFox_db <- dbConnect(RSQLite::SQLite(), 
                     "../WLF553_Arctic_Fox_Project/ArcticFox_db.db")

# Check sites table:
dbGetQuery(ArcticFox_db, "SELECT * FROM sites LIMIT 3;")

# Check individuals table:
dbGetQuery(ArcticFox_db, "SELECT * FROM individuals LIMIT 3;")

# Check site-year conditions table:
dbGetQuery(ArcticFox_db, "SELECT * FROM site_year_conditions LIMIT 3;")

# Check molt observations table:
dbGetQuery(ArcticFox_db, "SELECT * FROM molt_observations LIMIT 3;")
```


# 4: Visualizing Data Trends

Now that the data are cleaned up (duplicate information removed) and located in a relational database, we can begin to explore trends in the data with a few visualizations. First, we need to load the necessary `tidyverse` package and the `viridis` package for preferred colors. (We also need the `DBI` package, but that has already been loaded):

```{r loadvis, echo = TRUE, eval = TRUE, warning = FALSE}
library(tidyverse)
library(viridis)
```

Next, we need to connect to the Arctic fox database that we created:

```{r connect, echo = TRUE, eval = TRUE}
db_conn <- dbConnect(RSQLite::SQLite(), 
                     "../WLF553_Arctic_Fox_Project/ArcticFox_db.db")
```

And load the tables of interest:

```{r load_tables, echo = TRUE, eval = TRUE}
molt <- dbGetQuery(db_conn, "SELECT * FROM molt_observations;")
site_cond <- dbGetQuery(db_conn, "SELECT * FROM site_year_conditions;")
```

## General molt timing

I first wanted to get an idea of the general timing of molt through the season, across all the sites and years. At this point, as I tried to write the code using `ggplot`, I discovered that for some reason, the dates in my original data had been stored as functions. So I had to change them to numeric before I proceeded with `ggplot`. Additionally, I used the `fct_rev` function to reverse the order in which molt scores were displayed to match the chronology (heaviest winter coat to summer coat over the course of the season). Here is the code:

```{r molt_1, echo = TRUE, eval = TRUE}
  molt_plot1 <- molt 
  date_value <- as.numeric(molt$date)
  ggplot(data = molt, 
         mapping = aes(x = fct_rev(factor(moult_score)), y = date_value)) +
    geom_boxplot(fill = "darkseagreen") +
    labs(x = "Molt Score (% Winter Coat)",
         y = "Date (Jan 1 = 1)") +
    theme_light()
```

## Conditions across sites and years

Next, I wanted to get a feel for how temperature and snow conditions varied across sites and years. I plotted snow depth at each site and varied the color for temperature and the size of the points for continuity of snow. I broke the chart into separate plots for each year so that I could observe how conditions across the whole region varied from year to year:

```{r conditions_1, echo = TRUE, eval = TRUE}
 ggplot(data = site_cond,
         mapping = aes(x = factor(site_id), y = snow_depth, 
                       color = temperature, size = snow_continuous)) +
    geom_point() +
    facet_wrap(~ year) +
    labs(x = "Site", y = "Snow Depth (mm)", color = "Temperature (C)",
         size = "Snow Continuity (days)") +
    scale_color_viridis_c() +
    theme_light() +
    theme(axis.text.x = element_blank())
```

## Molt timing and environmental conditions

The previous plots enabled me to see that the years 2012 and 2015 were the coldest and had the most days of continuous snow. In contrast, 2011, 2014, and 2018 had warmer temperatures and fewer days of continuous snow. (2013, 2016, and 2017 were intermediate.) I wanted to see how the timing of the molt varied each year, so I needed to `left_join` to the site_cond table to pick up the year column. I chose just to look at the dates associated with mid-molt, or when the winter coat was at 50%, so I filtered for `moult_score == 50` before piping it into `ggplot`. The code is as follows:

```{r timing_1, echo = TRUE, eval = TRUE}
molt_plot2 <-  molt %>%
    left_join(site_cond) %>%
    filter(moult_score == 50) %>%
    ggplot(mapping = aes(x = factor(moult_score), y = date)) +
    geom_boxplot(fill = "darkseagreen") +
    facet_wrap(~ year) +
    labs(x = "",
         y = "Date at Mid-Molt (Jan 1 = 1)") +
    theme_light() +
    theme(axis.text.x = element_blank()) +
    theme(axis.ticks.x = element_blank())
  print(molt_plot2)
```

It is interesting to note that the timing of mid-molt is indeed later in the colder, snowier years of 2012 and 2015 and appears to be earlier in the warmer years, especially 2011, which had both warmer temperatures and little snow. This could be the basis of further statistical testing to see if these differences were significant. This dataset provides an interesting glimpse into the timing of the molt of Arctic foxes in relation to environmental conditions across several years and sites in Norway.
