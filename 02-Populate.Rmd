# Populating the database

Because data were originally in two large, unruly tables with much duplication, I needed to clean these up and select the right columns to go in each table. Fist, I needed to load the data (note that I had already created the csv for sites, as described in the next section):

```{r load_data, echo = TRUE, eval = FALSE}
# Load the data

phenology <- read.csv("../raw_data/morph_phenology.csv")
moult <- read.csv("../raw_data/seasonal_moulting_phenology.csv")
sites <- read.csv("../raw_data/sites.csv")
```

## Sites table

I created this table by hand in Excel, using the first three characters of each site name as its site ID. In the few cases where site names were partially repeated (such as north and south locations for Kjelsungbandet), I used the first two letters and the appropriate designation (e.g. KjS and KjN). (If I had not wanted these IDs to consist of part of the original names, I could have selected the sites column from one of the other tables, grouped by unique site names and then autogenerated an integer ID as the primary key).

The code for populating the sites table in the database was:

```{r sites, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "sites", sites, append = TRUE)
```

## Individuals table

This can be generated by selecting only the individual ID and morph columns from the seasonal_moulting_phenology.csv, and grouping by unique IDs. The code for this was as follows:

```{r indiv, echo = TRUE, eval = FALSE}
individuals <- phenology %>% 
  select(indiv_ID, morph)%>%
  distinct()
```

To populate the table in the database, I used this code:

```{r individuals, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "individuals", individuals, append = TRUE)
```

## Site-year conditions table

The phenology table was combined with the sites table using `left_join`, because I needed the site id. Then a new temporary column (site_id_year) was created with the `mutate` function to combine the site_id and year together. Next, I eliminated unwanted columns (morph, indiv_ID, start_95, median_50, end_0) using `select` and asked for `distinct` combinations of the remaining columns. Then, because I now had all the distinct site and year combinations, I could eliminate the site and temporary site_id_year columns with `select`and then `relocate` the columns in the preferred order. This leaves us with a table that contains the site_id, year, rodent, temperature, snow-depth, and snow_continuous. Here is the code:

```{r site_year, echo = TRUE, eval = FALSE}
site_year_conditions <- phenology %>%
  left_join(sites, by = "site") %>%
  mutate(site_id_year = paste(site_id, year, sep = "_")) %>%
  select(-c(morph, indiv_ID, start_95, median_50, end_0)) %>%
  distinct() %>%
  select(-site, -site_id_year) %>%
  relocate(site_id, .before = year)
```

I then populated the table in the database:

```{r site_year_conditions, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "site_year_conditions", 
             site_year_conditions, append = TRUE)
```

## Molt obeservations table

At this stage, I ran into the issue of having two different entities called "site_year_conditions," one of which was the temporary table I created, without a primary key, while the desired one was the table I populated in the database, including the primary key. In order to ensure that the latter would be what was referenced in my subsequent script, I ran the following code to overwrite the original object:

```{r overwrite, echo = TRUE, eval = FALSE}
site_year_conditions <- dbGetQuery(ArcticFox_db, 
                                   "SELECT * FROM site_year_conditions;")
```

I was then ready to create the molt observations table. This required me to `left_join` to both the sites table and the site_year_conditions table to pick up the necessary columns. I then used `select` to choose the desired columns, as follows:

```{r molt, echo = TRUE, eval = FALSE}
molt_observations <- moult %>%
  left_join(sites, by = "site") %>%
  left_join(site_year_conditions) %>%
  select(site_year_id, indiv_ID, date, moult_score)
```

To populate the table in the database, we can use:

```{r molt_observations, echo = TRUE, eval = FALSE}
dbWriteTable(ArcticFox_db, "molt_observations", 
             molt_observations, append = TRUE)
```

## Check the data

Now all four tables of the database are populated and ready to be used for data analysis. We can check to ensure that each of the tables was populated correctly with the following code:

```{r check, echo = TRUE, eval = TRUE}
# Confirm that package is loaded and connect to the database

library(DBI)

ArcticFox_db <- dbConnect(RSQLite::SQLite(), 
                     "../ArcticFox_db.db")

# Check sites table:
dbGetQuery(ArcticFox_db, "SELECT * FROM sites LIMIT 2;")

# Check individuals table:
dbGetQuery(ArcticFox_db, "SELECT * FROM individuals LIMIT 3;")

# Check site-year conditions table:
dbGetQuery(ArcticFox_db, "SELECT * FROM site_year_conditions LIMIT 3;")

# Check molt observations table:
dbGetQuery(ArcticFox_db, "SELECT * FROM molt_observations LIMIT 3;")
```

